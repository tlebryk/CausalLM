{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6vU1LSADZaaZ"},"outputs":[],"source":["# if running on colab, install the following packagess\n","# !pip install -U transformers datasets peft accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["\n","                <script type=\"application/javascript\" id=\"jupyter_black\">\n","                (function() {\n","                    if (window.IPython === undefined) {\n","                        return\n","                    }\n","                    var msg = \"WARNING: it looks like you might have loaded \" +\n","                        \"jupyter_black in a non-lab notebook with \" +\n","                        \"`is_lab=True`. Please double check, and if \" +\n","                        \"loading with `%load_ext` please review the README!\"\n","                    console.log(msg)\n","                    alert(msg)\n","                })()\n","                </script>\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# helps make your notebook pretty\n","import jupyter_black\n","\n","jupyter_black.load()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\tlebr\\miniconda3\\envs\\causal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from copy import copy\n","from time import time\n","from typing import Dict, List, Optional\n","\n","import pandas as pd\n","import torch\n","from datasets import Dataset\n","from peft import LoraConfig, get_peft_model\n","from peft.peft_model import PeftModelForCausalLM\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BatchEncoding,\n","    PreTrainedTokenizerBase,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from trl import DataCollatorForCompletionOnlyLM, SFTTrainer"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"c98oX9mvZYnw","outputId":"25c24583-81e7-417f-bdf7-017e72c0af8e"},"outputs":[],"source":["INSTRUCTION_TEMPLATE_BASE = \"\\n\\n### Human:\"\n","RESPONSE_TEMPLATE_BASE = \"\\n\\n### Assistant:\"\n","\n","\n","def load_model(\n","    model_name: str, peft_kwargs: Optional[Dict] = None\n",") -> PeftModelForCausalLM:\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","    if peft_kwargs is None:\n","        peft_kwargs = {}\n","    peft_config = LoraConfig(task_type=\"CAUSAL_LM\", **peft_kwargs)\n","    # alterantively, you can use the following to load the model\n","    # model = PeftModelForCausalLM.from_pretrained(model_name)\n","    model = get_peft_model(model, peft_config)\n","    return model\n","\n","\n","def add_special_tokens(\n","    example: Dict,\n","    tokenizer: PreTrainedTokenizerBase,\n",") -> Dict:\n","    # add eos_token before human text and bos_token before assistant text\n","    example[\"text\"] = (\n","        example[\"text\"]\n","        .replace(\n","            INSTRUCTION_TEMPLATE_BASE, tokenizer.eos_token + INSTRUCTION_TEMPLATE_BASE\n","        )\n","        .replace(RESPONSE_TEMPLATE_BASE, RESPONSE_TEMPLATE_BASE + tokenizer.bos_token)\n","    )\n","    if not example[\"text\"].endswith(tokenizer.eos_token):\n","        example[\"text\"] += tokenizer.eos_token\n","    # Remove leading EOS tokens\n","    while example[\"text\"].startswith(tokenizer.eos_token):\n","        example[\"text\"] = example[\"text\"][len(tokenizer.eos_token) :]\n","\n","    return example"]},{"cell_type":"markdown","metadata":{},"source":["We're going to use bloom-560m, a small multilingual model where we can reasonably finetune it without needing GPU. \n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"y7d6FjeLmLlH","outputId":"69985611-850e-4168-d8be-3bd33d70a314"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 5/5 [00:00<?, ? examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset_text=Dataset({\n","    features: ['text'],\n","    num_rows: 5\n","})\n","dataset_text[0]={'text': '\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant:<s> perro</s>'}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Preprocessing\n","model_name = \"bigscience/bloom-560m\"\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name, trust_remote_code=True, padding_side=\"right\"\n",")  # padding side should be right for causal lm\n","\n","# overfit to 5 examples\n","str1 = '\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant: perro'\n","str2 = '\\n\\n### Human: How do you say \"water\" in Spanish?\\n\\n### Assistant: agua'\n","str3 = '\\n\\n### Human: How do you say \"mother\" in Spanish?\\n\\n### Assistant: madre'\n","str4 = '\\n\\n### Human: How do you say \"hello\" in Spanish?\\n\\n### Assistant: hola'\n","str5 = '\\n\\n### Human: How do you say \"tree\" in Spanish?\\n\\n### Assistant: árbol'\n","train_data = {\n","    \"text\": [str1, str2, str3, str4, str5],\n","}\n","dataset_text = Dataset.from_dict(train_data)\n","dataset_text = dataset_text.map(lambda x: add_special_tokens(x, tokenizer))\n","print(f\"{dataset_text=}\")\n","print(f\"{dataset_text[0]=}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 5/5 [00:00<00:00, 308.06 examples/s]\n","Map: 100%|██████████| 5/5 [00:00<00:00, 1629.87 examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset=Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 5\n","})\n","dataset[0]['input_ids']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n","dataset[0]['labels']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n","dataset[0]['attention_mask']=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# tokenize the text\n","dataset = dataset_text.map(\n","    lambda example: tokenizer(example[\"text\"]), batched=True, remove_columns=[\"text\"]\n",")\n","# copy the input_ids to labels\n","dataset = dataset.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n","print(f\"{dataset=}\")\n","print(f\"{dataset[0]['input_ids']=}\")\n","print(f\"{dataset[0]['labels']=}\")\n","print(f\"{dataset[0]['attention_mask']=}\")"]},{"cell_type":"markdown","metadata":{},"source":["To start, labels and input_ids are identical. This means during training we will predict each token one at a time based on the previous tokens and learn by comparing what we predicted to the true label. Let's see what happens when we train a model like that. "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# training code inspired by\n","# https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html\n","\n","model = load_model(model_name)\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# How many times to iterate over the entire dataset\n","num_train_epochs = 15\n","\n","# We're not aligning the sequence length (ie padding or truncating)\n","# so batch training won't work for our toy example.\n","per_device_train_batch_size = 1\n","\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    seed=1,\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    train_dataset=dataset,\n","    args=training_arguments,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["holdout_str = (\n","    '\\n\\n### Human: How do you say \"good\" in Spanish?\\n\\n### Assistant:<s>'  # bueno\n",")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","holdout_input = tokenizer(holdout_str, return_tensors=\"pt\").to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def sample_generate(\n","    model: torch.nn.Module,\n","    tokenizer: PreTrainedTokenizerBase,\n","    inputs: BatchEncoding,\n","    **kwargs\n",") -> str:\n","    \"\"\"Runs tokenized text through the model and returns the generated text.\"\"\"\n","    outputs = model.generate(**inputs, **kwargs)\n","    gen_text = tokenizer.batch_decode(\n","        # strip the text of the prompt\n","        outputs[:, inputs[\"input_ids\"].shape[1] :]\n","    )\n","    return gen_text[0]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["ENGLISH_WORDS = [\"dog\", \"water\", \"mother\", \"hello\", \"tree\"]\n","SPANISH_WORDS = [\"perro\", \"agua\", \"madre\", \"hola\", \"árbol\"]\n","\n","\n","def predict_training_set(\n","    model: torch.nn.Module,\n","    tokenizer: PreTrainedTokenizerBase,\n","    english_words: List[str] = ENGLISH_WORDS,\n","    spanish_words: List[str] = SPANISH_WORDS,\n","):\n","    \"\"\"Runs predictions on the entire training set.\"\"\"\n","    for eng, span in zip(english_words, spanish_words):\n","        inputs2 = tokenizer(\n","            f'\\n\\n### Human: How do you say \"{eng}\" in Spanish?\\n\\n### Assistant:<s>',\n","            return_tensors=\"pt\",\n","        ).to(device)\n","        print(\n","            \"real answer:\",\n","            span,\n","            \"\\tpredicted answer:\",\n","            sample_generate(model, tokenizer, inputs2, max_new_tokens=5),\n","        )"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def print_iterative_generate(model, tokenizer, inputs):\n","    \"\"\"Approximates the training forward pass by iterating through a sequence\n","    and predicting one token at a time.\n","    \"\"\"\n","    tok_outputs = []\n","    for tok_id in range(1, len(inputs[\"input_ids\"][0]) + 1):\n","        iterative_inputs = inputs.copy()\n","        iterative_inputs[\"input_ids\"] = inputs[\"input_ids\"][:, :tok_id]\n","        iterative_inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, :tok_id]\n","        tok_outputs.append(\n","            sample_generate(model, tokenizer, iterative_inputs, max_new_tokens=1)\n","        )\n","    print(\"\".join(tok_outputs))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer: </s>\n","real answer: agua \tpredicted answer: </s>\n","real answer: madre \tpredicted answer: </s>\n","real answer: hola \tpredicted answer: </s>\n","real answer: árbol \tpredicted answer: </s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["'</s>'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["original_output = sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)\n","original_output"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["#include\n"," code\n"," to I get \"it's\" to a?\n","A: Spanish: How</s>\n"]}],"source":["print_iterative_generate(model, tokenizer, holdout_input)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:10<00:00,  1.06it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 70.8013, 'train_samples_per_second': 1.059, 'train_steps_per_second': 1.059, 'train_loss': 4.480442301432292, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["training1 = trainer.train()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer: </s>\n","real answer: agua \tpredicted answer: </s>\n","real answer: madre \tpredicted answer: mamá</s>\n","real answer: hola \tpredicted answer: </s>\n","real answer: árbol \tpredicted answer: tree</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["We're still kind of confused. Let's continue training. "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/75 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:06<00:00,  1.12it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 66.6733, 'train_samples_per_second': 1.125, 'train_steps_per_second': 1.125, 'train_loss': 2.1419362386067706, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=75, training_loss=2.1419362386067706, metrics={'train_runtime': 66.6733, 'train_samples_per_second': 1.125, 'train_steps_per_second': 1.125, 'train_loss': 2.1419362386067706, 'epoch': 15.0})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n","real answer: agua \tpredicted answer:  agua</s>\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["' bueno</s>'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)"]},{"cell_type":"markdown","metadata":{},"source":["After 30 epochs, we learned what we were supposed to!"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["#\n",": How do you say \"how morning in Spanish?\n","\n","### Assistant: gu bueno\n"]}],"source":["print_iterative_generate(model, tokenizer, holdout_input)"]},{"cell_type":"markdown","metadata":{},"source":["As we expected, we learned how to write the whole prompt (better). But we kinda don't care if we're focused on building a chat bot. Is there a way to learn just the chat part? "]},{"cell_type":"markdown","metadata":{},"source":["### Masked approach\n","Ok so we're wasting time learning the human prompt. Let's switch up the labels to only focus on the prompt. Let's exploit the bos and eos tokens we added to the text earlier. "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def create_special_mask(example: Dict) -> Dict:\n","    \"\"\"Mask human text and keep assistant text as it is.\n","\n","    Args:\n","        example (Dict): Result of tokenizing some text\n","\n","    Returns:\n","        Dict: The dict with the label masked\n","    \"\"\"\n","    # setting a token to -100 is how we \"mask\" a token\n","    # and tell the model to ignore it when calculating the loss\n","    mask_token_id = -100\n","    # assume we always start with a human text\n","    human_text = True\n","    for idx, tok_id in enumerate(example[\"labels\"]):\n","        if human_text:\n","            # mask all human text up until and including the bos token\n","            example[\"labels\"][idx] = mask_token_id\n","            if tok_id == tokenizer.bos_token_id:\n","                human_text = False\n","        elif not human_text and tok_id == tokenizer.eos_token_id:\n","            human_text = True\n","        elif not human_text:\n","            # leave example['labels'] text as it is when assistant text\n","            continue\n","    return example"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 5/5 [00:00<00:00, 1222.47 examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset_masked[0][\"labels\"]=tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,     2])\n","non masked text:  perro</s>\n","full 'label': <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> perro</s>\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# sanity check that our format is correct\n","# we'd expect -100 for the human text and the actual token(s) for the assistant text\n","# mask human characters but keep the assistant text as it is\n","dataset_masked = dataset.map(create_special_mask)\n","# convert dataset from lists to torch tensors\n","dataset_masked.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","print(f\"{dataset_masked[0][\"labels\"]=}\")\n","label_ex = dataset_masked[0][\"labels\"]\n","# let's see just the non-masked text\n","print(f\"non masked text: {tokenizer.decode(label_ex[label_ex != -100], skip_special_tokens=False)}\")\n","# let's see just the masked text\n","# -100 is not a real token, convert to something the tokenizer understands\n","label_ex[label_ex == -100] = 0\n","print(f\"full 'label': {tokenizer.decode(label_ex, skip_special_tokens=False)}\")\n","\n","# )"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Reset the model\n","model = load_model(model_name)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"eU_tLxzYjwoR"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    train_dataset=dataset_masked,\n","    args=training_arguments,\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3BLC2KpbceK-","outputId":"695dd535-c8ed-4db6-e85b-89cf581ae224"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/75 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:01<00:00,  1.22it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 61.7164, 'train_samples_per_second': 1.215, 'train_steps_per_second': 1.215, 'train_loss': 1.8535097249348957, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["training2 = trainer.train()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training2.metrics['train_runtime']=61.7164\n","training1.metrics['train_runtime'] =70.8013\n","13.0%\n"]}],"source":["print(f\"{training2.metrics['train_runtime']=}\")\n","print(f\"{training1.metrics['train_runtime'] =}\")\n","print(\n","    f\"{100*round((training1.metrics['train_runtime']  - training2.metrics['train_runtime']) / training1.metrics['train_runtime'] , 2)}%\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We were faster this time by more than 10%. Presumably, the fact that we have fewer loss calculations makes things a bit quicker. \n","I wouldn't bank on this speed up being this large - our example is pretty lopsided with much more human text than generated text. But when training times are in the hours, every little percentage matters.  "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n"]},{"name":"stdout","output_type":"stream","text":["real answer: agua \tpredicted answer:  agua</s>\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["' bueno</s>'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)"]},{"cell_type":"markdown","metadata":{},"source":["Cool, this time we only needed 15 epochs to learn the task. Let's go back to how things are under the hood during training "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["#include\n"," code\n"," to I get \"we\" in English?\n","A: Spanish: How bueno\n"]}],"source":["print_iterative_generate(model, tokenizer, holdout_input)"]},{"cell_type":"markdown","metadata":{},"source":["Iteratively predicting the prompt leads to non-sense compared with our first training approach. This checks out: we masked the prompt during training and therefore don’t learn how to predict anything up until our real target: the assistant response."]},{"cell_type":"markdown","metadata":{},"source":["### Finetuning using HuggingFace TRL"]},{"cell_type":"markdown","metadata":{},"source":["Let's use a pipeline to have huggingface do this for us use the collator, use the parser for the assitant and human text "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# reset the model\n","model = load_model(model_name)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# a huggingface function to do the copying of labels for you.\n","# using the insttruction and response templates will mask everything between the instruction template\n","# and the start of the response_template\n","collator = DataCollatorForCompletionOnlyLM(\n","    instruction_template=tokenizer.eos_token,\n","    response_template=tokenizer.bos_token,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\tlebr\\miniconda3\\envs\\causal\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map: 100%|██████████| 5/5 [00:00<00:00, 1248.38 examples/s]\n"]}],"source":["# a beefed up trainer class for supervised fine-tuning.\n","# we can feed the raw text and have it run the DataCollatorForCompletionOnlyLM on that\n","# test to get the text data in the tokenized format causallm models expect\n","trainersft = SFTTrainer(\n","    model,\n","    train_dataset=dataset_text,\n","    dataset_text_field=\"text\",\n","    data_collator=collator,\n","    args=training_arguments,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let's quickly check what the collator does on a single example. Note the brackets: DataCollatorForCompletionOnlyLM expects a list of examples. One upside is that it will handle truncation and padding so you can batch efficiently. "]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","### Human: How do you say \"dog\" in Spanish?\n","\n","### Assistant:<s> perro</s>\n"]},{"data":{"text/plain":["{'input_ids': tensor([[   603, 105311,  22256,     29,   7535,    727,   1152,   5894,  20587,\n","            744,      5,    361,  49063,   7076, 105311, 143005,     29,      1,\n","          82208,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,  -100]])}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["print(dataset_text[0][\"text\"])\n","collator_output = collator([tokenizer(dataset_text[0][\"text\"])])\n","collator_output"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|▏         | 1/75 [00:00<01:09,  1.06it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:00<00:00,  1.23it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 60.8437, 'train_samples_per_second': 1.233, 'train_steps_per_second': 1.233, 'train_loss': 2.96381591796875, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["sftrain = trainersft.train()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sftrain.metrics['train_runtime']=60.8437\n","training1.metrics['train_runtime'] =70.8013\n","14.000000000000002%\n"]}],"source":["print(f\"{sftrain.metrics['train_runtime']=}\")\n","print(f\"{training1.metrics['train_runtime'] =}\")\n","print(\n","    f\"{100*round((training1.metrics['train_runtime']  - sftrain.metrics['train_runtime']) / training1.metrics['train_runtime'] , 2)}%\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Training took longer with the TRL approach. This might be credited to the fact that we have to tokenize at training time rather than as a preprocessing step in the masked approach. Remember that this approach gives us free batching (you’d need to tweak the tokenization process to use the masked approach to batch properly), which should make things faster in the long run. \n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n"]},{"name":"stdout","output_type":"stream","text":["real answer: agua \tpredicted answer:  agua</s>\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["' bueno</s>'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["#include\n"," code\n"," to I get \"we\" in a?\n","A: Spanish: How bueno\n"]}],"source":["print_iterative_generate(model, tokenizer, holdout_input)"]},{"cell_type":"markdown","metadata":{},"source":["If we look closely at the sample output from the data collator, our labels looked like this: \n","\n","`tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,  -100]])`\n","This is slightly different than our manual masking approach:\n","\n","`tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,     2])`\n","Ignoring the slightly different tensor shape, we have an additional *2* token with the manual approach. This is because the data collator does exclusive masking of the instruction template. We hacked it to say that the \"\\</s>\" character is the \"instruction template\" which signals the human is about to be the source of the text. We generally don't want to generate that actual instruction template, so the collator masks it. We still correctly outputed the end of sequence token, but that was luck not actual learning. If you look back at the pretraining output, we always knew to end our sequences with \\</s> even before we finetuned the model on our task. In fact, if we continue training, we forget that ability to end sequences with an end of sequence token."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 25/25 [00:21<00:00,  1.15it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 21.807, 'train_samples_per_second': 1.146, 'train_steps_per_second': 1.146, 'train_loss': 0.39307949066162107, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=25, training_loss=0.39307949066162107, metrics={'train_runtime': 21.807, 'train_samples_per_second': 1.146, 'train_steps_per_second': 1.146, 'train_loss': 0.39307949066162107, 'epoch': 5.0})"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["argcopy = copy(trainersft.args)\n","argcopy.num_train_epochs = 5\n","trainersft.args = argcopy\n","trainersft.train()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n","real answer: agua \tpredicted answer:  agua\n","### Assistant:\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola\n","### Human:\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["### Using special tokens to split the human and assistant text"]},{"cell_type":"markdown","metadata":{},"source":["How might we fix this problem?\n","\n","One solution is to use the instruction and response template more as originally intended where they aren't the bos and eos tokens but rather actual text indicating whether it's the human or assistant speaking. Let's try using \"\\n\\n Human:\" and \"\\n\\n Assistant:\" as our instruciton and response templates respectively. "]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["collator = DataCollatorForCompletionOnlyLM(\n","    instruction_template=INSTRUCTION_TEMPLATE_BASE,\n","    response_template=RESPONSE_TEMPLATE_BASE,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RESPONSE_TEMPLATE_BASE='\\n\\n### Assistant:'\n","tokenizer(RESPONSE_TEMPLATE_BASE)={'input_ids': [603, 105311, 143005, 29], 'attention_mask': [1, 1, 1, 1]}\n"]}],"source":["print(f\"{RESPONSE_TEMPLATE_BASE=}\")\n","print(f\"{tokenizer(RESPONSE_TEMPLATE_BASE)=}\")"]},{"cell_type":"markdown","metadata":{},"source":["How to interpret this: We're expecting `\\n\\n### Assistant:` towards the end of our text to signal that this is the model's output. Specifically, we should see the pattern of tokens: [603, 105311, 143005, 29]."]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset_text[\"text\"][0]='\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant:<s> perro</s>'\n","tokenizer(dataset_text[\"text\"][0])={'input_ids': [603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["print(f\"{dataset_text[\"text\"][0]=}\")\n","print(f\"{tokenizer(dataset_text[\"text\"][0])=}\")"]},{"cell_type":"markdown","metadata":{},"source":["We do see \"\\n\\n### Assistant:\" in the text, but rather than [**603**, 105311, 143005, 29], we get: [**7076**, 105311, 143005, 29].\n","What gives?\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["'\\n\\n'"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# desired token\n","tokenizer.decode(603)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["'?\\n\\n'"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# token from the real string\n","tokenizer.decode(7076)"]},{"cell_type":"markdown","metadata":{},"source":["What happens when our tokenizer acts up like this? "]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[   603, 105311,  22256,     29,   7535,    727,   1152,   5894,  20587,\n","            744,      5,    361,  49063,   7076, 105311, 143005,     29,      1,\n","          82208,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100]])}\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\tlebr\\miniconda3\\envs\\causal\\Lib\\site-packages\\trl\\trainer\\utils.py:164: UserWarning: Could not find response key `\n","\n","### Assistant:` in the following instance: \n","\n","### Human: How do you say \"dog\" in Spanish?\n","\n","### Assistant:<s> perro</s> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n","  warnings.warn(\n","c:\\Users\\tlebr\\miniconda3\\envs\\causal\\Lib\\site-packages\\trl\\trainer\\utils.py:179: UserWarning: Could not find instruction key `\n","\n","### Human:` in the following instance: \n","\n","### Human: How do you say \"dog\" in Spanish?\n","\n","### Assistant:<s> perro</s> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n","  warnings.warn(\n"]}],"source":["no_special_tokens = collator([tokenizer(dataset_text[\"text\"][0])])\n","print(no_special_tokens)"]},{"cell_type":"markdown","metadata":{},"source":["_This instance will be ignored in loss calculation_\n","That's a rather ominous warnings. \n","\n","If we inpsect the transformed labels we get: \n","```\n","'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100, -100, -100, -100, -100]]).\n","```\n","Basically, if we moved forward, we'd be masking everything so during training we will learn nothing. That's not great. What should we do?\n","\n","You might be tempted to finagle with formatting until you hit sucess. Not a bad impulse, but contexualized tokenization is a dangerous rabbit hole. See what happens when we try some straightforward fixes: "]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 80379, 105311, 143005, 29, 1, 82208, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["# Try adding a space between the question mark and response template\n","print(f\"{tokenizer(dataset_text[\"text\"][0].replace(\"?\\n\\n\", \"? \\n\\n\"))}\")\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["'? \\n\\n'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# We got: [80379, 105311, 143005, 29,]\n","tokenizer.decode(80379)"]},{"cell_type":"markdown","metadata":{},"source":["In short: not only is `?\\n\\n` part of the vocabulary, thus breaking our response_template, but so is `'? \\n\\n'`. I'll stop with the experimenting there and you'll have to trust me that continuning to finagle the formatting will only end in anguish. "]},{"cell_type":"markdown","metadata":{},"source":["The more stable solution is to add your special token directly to the tokenizer. "]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[250680,   7535,    727,   1152,   5894,  20587,    744,      5,    361,\n","          49063,     34, 250681,      1,  82208,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100, 82208,     2]])}\n"]}],"source":["# add the special tokens we're looking for to the vocabulary\n","model = load_model(model_name)\n","tokenizer2 = AutoTokenizer.from_pretrained(\n","    model_name, trust_remote_code=True, padding_side=\"right\"\n",")\n","num_added_toks = tokenizer2.add_special_tokens(\n","    {\"additional_special_tokens\": [INSTRUCTION_TEMPLATE_BASE, RESPONSE_TEMPLATE_BASE]}\n",")\n","\n","# best practice to prepare the model for the new tokens.\n","model.resize_token_embeddings(len(tokenizer2))\n","\n","collator = DataCollatorForCompletionOnlyLM(\n","    instruction_template=INSTRUCTION_TEMPLATE_BASE,\n","    # let's add the bos_token because we don't really need to learn that\n","    # it should always be after the base response template\n","    response_template=RESPONSE_TEMPLATE_BASE + tokenizer2.bos_token,\n","    tokenizer=tokenizer2,\n",")\n","\n","\n","with_special_tokens = collator([tokenizer2(dataset_text[\"text\"][0])])\n","print(with_special_tokens)"]},{"cell_type":"markdown","metadata":{},"source":["Ran with no warnings! If we look at labels, we can see the last tokens are unmasked as we want. "]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RESPONSE_TEMPLATE_BASE='\\n\\n### Assistant:'\n","tokenizer2(RESPONSE_TEMPLATE_BASE)={'input_ids': [250681], 'attention_mask': [1]}\n","str1='\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant: perro'\n","tokenizer2(str1)={'input_ids': [250680, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 34, 250681, 82208], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["# sanity check\n","print(f\"{RESPONSE_TEMPLATE_BASE=}\")\n","print(f\"{tokenizer2(RESPONSE_TEMPLATE_BASE)=}\")\n","print(f\"{str1=}\")\n","print(f\"{tokenizer2(str1)=}\")"]},{"cell_type":"markdown","metadata":{},"source":["After we added the special tokens, the response_template is encoded as a single token: **250681**, which is now how it gets tokenized in the actual string example: \n","\n","'input_ids': [250680, 7535, 727, 1152, 5894, 567, 1988, 182448, 361, 49063, 34, **250681**, 82208]\n","Sucess!"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["holdout_input2 = tokenizer2(holdout_str, return_tensors=\"pt\").to(device)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 5/5 [00:00<00:00, 1677.05 examples/s]\n"]}],"source":["trainersft = SFTTrainer(\n","    model,\n","    train_dataset=dataset_text,\n","    dataset_text_field=\"text\",\n","    data_collator=collator,\n","    args=training_arguments,\n","    max_seq_length=512,\n","    tokenizer=tokenizer2,\n",")"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/75 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:01<00:00,  1.22it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 61.5561, 'train_samples_per_second': 1.218, 'train_steps_per_second': 1.218, 'train_loss': 3.4137898763020833, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["spec_tok_sft = trainersft.train()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  Dog</s>\n","real answer: agua \tpredicted answer: water</s>\n","real answer: madre \tpredicted answer: mamá</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer: tree</s>\n"]}],"source":["predict_training_set(model, tokenizer2)"]},{"cell_type":"markdown","metadata":{},"source":["We didn't learn the output properly! One cost of creating new special tokens is that the model has never seen them before and needs to learn them from scratch. Let's train a bit longer to see if we can figure it out. "]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/75 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [01:00<00:00,  1.23it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 60.9864, 'train_samples_per_second': 1.23, 'train_steps_per_second': 1.23, 'train_loss': 0.5193242390950521, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=75, training_loss=0.5193242390950521, metrics={'train_runtime': 60.9864, 'train_samples_per_second': 1.23, 'train_steps_per_second': 1.23, 'train_loss': 0.5193242390950521, 'epoch': 15.0})"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["trainersft.train()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n"]},{"name":"stdout","output_type":"stream","text":["real answer: agua \tpredicted answer:  agua</s>\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer2)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["' buenas</s>'"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["sample_generate(model, tokenizer2, holdout_input2, max_new_tokens=5)"]},{"cell_type":"markdown","metadata":{},"source":["We're close enough on the holdout case. However, we can see the tradeoff when we add instruction and response tokens. We now properly predict the end of sequence token, but we had to see more data and therefore train for longer to learn the new special tokens. Fortunately, many chat models like LLAMA-2-chat will already have specific instruction and response tokens. "]},{"cell_type":"markdown","metadata":{},"source":["### Multi-turn example"]},{"cell_type":"markdown","metadata":{},"source":["A core functionality of modern chatbots is the ability to remember previous interactions. Up to until now, we've trained off of individual human/assistant interactions. Let's briefly try learning a very simple multi-turn conversation to make sure our approach holds up. "]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 3/3 [00:00<00:00, 986.51 examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset_text_multi[0]={'text': '\\n\\n### Human: How do you say \"water\" in Spanish?\\n\\n### Assistant:<s> agua</s>\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant:<s> perro</s>'}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model = load_model(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name, trust_remote_code=True, padding_side=\"right\"\n",")\n","# concatenate human:assistant interactions\n","multi_turn_training_data = {\n","    \"text\": [str2 + str1, str3 + str4, str5],\n","}\n","dataset_text_multi = Dataset.from_dict(multi_turn_training_data)\n","dataset_text_multi = dataset_text_multi.map(lambda x: add_special_tokens(x, tokenizer))\n","print(f\"{dataset_text_multi[0]=}\")"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 3/3 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 3/3 [00:00<00:00, 182.39 examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset_multi=Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 3\n","})\n","dataset_multi[0]['input_ids']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 567, 37636, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 14423, 2, 603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n","dataset_multi[0]['labels']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 567, 37636, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 14423, 2, 603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# tokenize the text\n","dataset_multi = dataset_text_multi.map(\n","    lambda example: tokenizer(example[\"text\"]), batched=True, remove_columns=[\"text\"]\n",")\n","# copy the input_ids to labels\n","dataset_multi = dataset_multi.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n","print(f\"{dataset_multi=}\")\n","print(f\"{dataset_multi[0]['input_ids']=}\")\n","print(f\"{dataset_multi[0]['labels']=}\")"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 3/3 [00:00<00:00, 842.85 examples/s]"]},{"name":"stdout","output_type":"stream","text":["dataset_multi[0][\"labels\"]=tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14423,     2,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,     2])\n","non masked text:  agua</s> perro</s>\n","full 'label': <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> agua</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> perro</s>\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# let's test our special characters method\n","dataset_multi = dataset_multi.map(create_special_mask)\n","# convert dataset from lists to torch tensors\n","dataset_multi.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","print(f\"{dataset_multi[0][\"labels\"]=}\")\n","label_ex = dataset_multi[0][\"labels\"]\n","# let's see just the non-masked text\n","print(f\"non masked text: {tokenizer.decode(label_ex[label_ex != -100], skip_special_tokens=False)}\")\n","label_ex[label_ex == -100] = 0\n","print(f\"full 'label': {tokenizer.decode(label_ex, skip_special_tokens=False)}\")\n"," "]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    train_dataset=dataset_multi,\n","    args=training_arguments,\n",")"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 45/45 [00:51<00:00,  1.15s/it]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 51.605, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.872, 'train_loss': 2.0485241360134547, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=45, training_loss=2.0485241360134547, metrics={'train_runtime': 51.605, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.872, 'train_loss': 2.0485241360134547, 'epoch': 15.0})"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer: </s>\n","real answer: agua \tpredicted answer: </s>\n","real answer: madre \tpredicted answer: madre</s>\n","real answer: hola \tpredicted answer: </s>\n","real answer: árbol \tpredicted answer: tree</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["We haven't learned the task properly. Multi-turn chat is a moderately more complex task and the number of examples per epoch is down because we've concatenated pieces together. Let's keep training. "]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/45 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 45/45 [00:51<00:00,  1.14s/it]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 51.4634, 'train_samples_per_second': 0.874, 'train_steps_per_second': 0.874, 'train_loss': 0.4767167833116319, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=45, training_loss=0.4767167833116319, metrics={'train_runtime': 51.4634, 'train_samples_per_second': 0.874, 'train_steps_per_second': 0.874, 'train_loss': 0.4767167833116319, 'epoch': 15.0})"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["real answer: perro \tpredicted answer:  perro</s>\n","real answer: agua \tpredicted answer:  agua</s>\n","real answer: madre \tpredicted answer:  madre</s>\n","real answer: hola \tpredicted answer:  hola</s>\n","real answer: árbol \tpredicted answer:  árbol</s>\n"]}],"source":["predict_training_set(model, tokenizer)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["' bueno</s>'"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)"]},{"cell_type":"markdown","metadata":{},"source":["Sucess! "]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","Obviously our data is dumbed down and our task is very simple. However, as a proof of concepts of sorts, we've learned a few tricks to finetune CausalLM models, ranging from manually copying input_ids, to manually masking labels, to using the TRL library to mask labels using bos/eos tokens, to adding special tokens to play nicely with the TRL modules.  "]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
